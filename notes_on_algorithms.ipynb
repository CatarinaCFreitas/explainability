{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explainable AI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook summarizes relevant information regarding some state-of-the-art algorithms used to explain ML models, including:\n",
    "\n",
    "- <a href='#ebm'>Explainable Boosting Machine (EBM)</a>\n",
    "- <a href='#lime'>Local Interpretable Model-Agnostic Explanations (LIME)</a>\n",
    "- <a href='#shap'>SHapley Additive exPlanations (SHAP)</a>\n",
    "- <a href='#pdm'>Partial dependence model</a>\n",
    "- <a href='#morris'>Morris Sensitivity</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ebm'></a>\n",
    "\n",
    "## Explainable Boosting Machine (EBM)\n",
    "\n",
    "\n",
    "- Developed at Microsoft Research\n",
    "\n",
    "\n",
    "- Designed to have accuracy comparable to state-of-the-art methods like Random Forest and Boosted Trees, while being highly intelligibile and explainable\n",
    "\n",
    "\n",
    "- EBM is a generalized additive model (GAM) with a few major improvements over traditional GAMs:\n",
    "    - Standard GAMs usually model the dependent variable as a sum of univariate models; their accuracy is significantly less than more complex models that permit interactions between variables\n",
    "    - EBM can automatically detect and include pairwise interaction terms \n",
    "    - EBM learns each feature function using bagging and gradient boosting\n",
    "    - Because EBM is an additive model, each feature contributes to predictions in a modular way that makes it easy to reason about the contribution of each feature to the prediction\n",
    "\n",
    "\n",
    "Algorithm explanation: https://www.youtube.com/watch?v=MREiHgHgl0k&ab_channel=MicrosoftDeveloper  \n",
    "EBM is a fast implementation of the GA2M algorithm: https://dl.acm.org/doi/10.1145/2487575.2487579\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='lime'></a>\n",
    "\n",
    "## Local Interpretable Model-Agnostic Explanations (LIME)\n",
    "\n",
    "\n",
    "In order to figure out what parts of the input are contributing to the prediction, LIME tests what happens to the predictions when you give variations of your data to your machine learning model. \n",
    "\n",
    "\n",
    "**How LIME works:**\n",
    "\n",
    "1. You train a black-box model and select the instance for which you want an explanation\n",
    "2. LIME Generates a new dataset consisting of permuted samples from your data and the corresponding predictions from your black-box model as targets\n",
    "3. Perturbs your dataset (adds noise to continuous features, removes words in text, hides parts of images)\n",
    "4. Trains an interpretable model on the new dataset, weighting the perturbed data points by their proximity to the original example\n",
    "5. Explains the prediction by interpreting this new model\n",
    "\n",
    "\n",
    "<img src=\"images/lime1.png\" width=\"600\" height=\"600\" align=\"center\"/>\n",
    "<br clear=\"all\" />\n",
    "\n",
    "<font color='gray'>Image source: [4]</font>\n",
    "\n",
    "\n",
    "<img src=\"images/lime2.png\" width=\"800\" height=\"800\" align=\"center\"/>\n",
    "<br clear=\"all\" />\n",
    "\n",
    "<font color='gray'>Image source: [5]</font>\n",
    "\n",
    "\n",
    "**Problems:**\n",
    "1. Instability of the explanations:\n",
    "    - Explanations of similar examples might be totally different\n",
    "    - If you repeat the sampling process, then the explanations that come out can be different\n",
    "2. LIME suffers from labels and data shift (training and test distributions are different)\n",
    "    - Instances generated by LIME’s perturbation procedure are significantly different from training instances drawn from the underlying distribution\n",
    "3. Explanation depends on the choice of LIME hyperparameters\n",
    "\n",
    "\n",
    "**Note:**\n",
    "The explanations created with local surrogate models can use other (interpretable) features than the original model was trained on (e.g. a regression model could be trained on components of a principal component analysis (PCA), but LIME might be trained on the original data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='shap'></a>\n",
    "\n",
    "## SHapley Additive exPlanations (SHAP)\n",
    "\n",
    "\n",
    "SHAP can explain any machine learning model. It is based on Shapley values, a concept coming from game theory. It's goal is to explain the prediction of an instance by computing the contribution of each feature to the prediction. \n",
    "\n",
    "For example, imagine that we have 3 features in a model to predict income: Age, Gender, and Job. Shapley quantifies the contribution that each feature brings to the model. It is based on the idea that the outcome of each possible combination of features should be used to get the importance of one single feature. So, to calculate the contribution of feature Age, we need to consider all the interactions of Age with the other features and their collective contribution as well.\n",
    "\n",
    "The SHAP value of Age will be the (weighted) average of the marginal contributions of this feature. A marginal contribution of a feature is the difference between predictions by including the feature and without including the feature. \n",
    "\n",
    "<img src=\"images/shap2.png\" width=\"500\" height=\"500\" align=\"center\"/>\n",
    "<br clear=\"all\" />\n",
    "\n",
    "<font color='gray'>Image source: [10]</font>\n",
    "\n",
    "The SHAP values can show how much each predictor contributes, either positively or negatively, to the target variable. Summing the SHAP values of each feature of a given observation yields the difference between the prediction of the model and the average prediction for the dataset.\n",
    "\n",
    "<img src=\"images/shap1.png\" width=\"800\" height=\"800\" align=\"center\"/>\n",
    "<br clear=\"all\" />\n",
    "\n",
    "<font color='gray'>Image source: [11]</font>\n",
    "\n",
    "Considering every feature and their interactions, if F is the total number of features, SHAP requires to train 2^F distinct predictive models. As F increases, this becames inapplicable, so SHAP libraries employ approximations and samplings that make this feasible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='pdm'></a>\n",
    "\n",
    "## Partial dependence model\n",
    "\n",
    "- A partial dependence plot shows the global effect of a feature on the outcome of a ML model. \n",
    "\n",
    "- The partial dependence function for a feature at a value represents the average prediction if we have all data points assume that feature value.\n",
    "\n",
    "<img src=\"images/pdp.png\" width=\"600\" height=\"600\" align=\"center\"/>\n",
    "<br clear=\"all\" />\n",
    "\n",
    "<font color='gray'>Image source: [11]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='morris'></a>\n",
    "\n",
    "## Morris Sensitivity\n",
    "\n",
    " - Morris method is a statistical method for global sensitivity analysis (SA)\n",
    " - It is a one-step-at-a-time method (OAT), meaning that in each run only one input parameter is given a new value \n",
    " - In OAT SA design, all input variables in question are changed by the same relative amount. However, the Morris method takes into account changing only one variable between a pair of model simulations\n",
    " - Identification and ranking of the important variables are done on the basis of the difference computed between a pair of model simulations\n",
    "\n",
    "- The input space is considered as a grid. The algorithm starts at a randomly chosen point in the K-dimensional space and creates a trajectory through all the K-dimensional variable space.\n",
    "\n",
    "<img src=\"images/morris1.png\" width=\"600\" height=\"600\" align=\"center\"/>\n",
    "<br clear=\"all\" />\n",
    "\n",
    "<font color='gray'>Image source: [14]</font>\n",
    "\n",
    "- This method captures output variation when one of the trajectory points is moved to one of its closest neighbors. This variation is called an elementary effect:\n",
    "\n",
    "<img src=\"images/morris2.png\" width=\"600\" height=\"600\" align=\"center\"/> \n",
    "<br clear=\"all\" />\n",
    "  \n",
    "*i = 1,..., R (R trajectories)  \n",
    "j = 1,..., K (K dimensions)*  \n",
    "\n",
    "- A certain number of trajectories R are generated, in order to observe the consequence of elementary effects anywhere in the input space.\n",
    "\n",
    "- Finally, these elementary effects are used to estimate global sensitivity with the following indicators: mu, mu_star, sigma.\n",
    "\n",
    "<br clear=\"all\" />\n",
    "\n",
    "**Summary statistics on simulated elementary effects:**\n",
    "\n",
    "**Mu**:\n",
    "- The mean assesses the overall impact (influence) of the feature on the model prediction\n",
    "    - A large mean indicates large changes in the response when perturbing a feature (an influential parameter)\n",
    "- Note that it can aggregate very different strengths of effects and cancel opposite effects\n",
    "    \n",
    "**Mu_star**:\n",
    "\n",
    "<img src=\"images/morris3.png\" width=\"150\" height=\"150\" align=\"center\"/>\n",
    "<br clear=\"all\" />\n",
    "\n",
    "- A more robust indicator, it is computed as the mean of the absolute value of the elementary effects\n",
    "    \n",
    "    \n",
    "**Sigma**:\n",
    "\n",
    "<img src=\"images/morris2.png\" width=\"600\" height=\"600\" align=\"center\"/>\n",
    "<br clear=\"all\" />\n",
    "\n",
    "- The standard deviation or variance is a measure on non-linear and/or interaction effects:\n",
    "    - A large value indicates that the influence of the feature depends highly on the interaction with other features\n",
    "    - A small value indicates similar influences for different sample points, implying that the effect of the feature is independent of its interaction with other features\n",
    "        \n",
    "   \n",
    "        \n",
    "<font color='gray'>Equations source: [14]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources\n",
    "\n",
    "EBM  \n",
    "[1] https://medium.com/analytics-vidhya/model-interpretation-with-microsofts-interpret-ml-85aa0ad697ae  \n",
    "[2] https://arxiv.org/pdf/1909.09223.pdf\n",
    "\n",
    "LIME  \n",
    "[3] https://homes.cs.washington.edu/~marcotcr/blog/lime/  \n",
    "[4] https://christophm.github.io/interpretable-ml-book/lime  \n",
    "[5] https://www.oreilly.com/content/introduction-to-local-interpretable-model-agnostic-explanations-lime/  \n",
    "[6] https://towardsdatascience.com/understanding-model-predictions-with-lime-a582fdff3a3b  \n",
    "[7] https://towardsdatascience.com/whats-wrong-with-lime-86b335f34612  \n",
    "\n",
    "SHAP  \n",
    "[8] https://christophm.github.io/interpretable-ml-book/shap.html  \n",
    "[9] https://medium.com/@prarthana1/unboxing-machine-learning-models-with-shap-for-better-interpretation-5b2caa81e3da   \n",
    "[10] https://towardsdatascience.com/shap-explained-the-way-i-wish-someone-explained-it-to-me-ab81cc69ef30  \n",
    "[11] https://towardsdatascience.com/three-model-explanability-methods-every-data-scientist-should-know-c332bdfd8df  \n",
    "\n",
    "Partially dependence model  \n",
    "[11]\n",
    "\n",
    "Morris sensitivity  \n",
    "[12] https://www.youtube.com/watch?v=P8Rfipkid3w&ab_channel=JefCaers  \n",
    "[13] https://openmole.org/Sensitivity.html  \n",
    "[14] https://link.springer.com/article/10.1007/s11600-019-00356-5   \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
