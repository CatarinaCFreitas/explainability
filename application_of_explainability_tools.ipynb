{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explainable AI\n",
    "\n",
    "This notebook contains experiments on AI explainability using the following projects:\n",
    "\n",
    "- 1. <a href='#intrepret_ml'>Intrepret ML</a>  \n",
    "- 2. <a href='#intrepret_ml_community'>Intrepret ML Community</a> (has extra features)\n",
    "- 3. <a href='#lime'>LIME</a>\n",
    "- 4. <a href='#shap'>SHAP</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\n",
    "    'status_checking_account',\n",
    "    'duration_in_months',\n",
    "    'credit_history',\n",
    "    'purpose',\n",
    "    'credit_amount',\n",
    "    'savings_account_or_bonds',\n",
    "    'present_employment_since',\n",
    "    'installment_rate_in_percentage_of_disposable_income',\n",
    "    'personal_status_and_sex',\n",
    "    'other_debtors_guarantors',\n",
    "    'present_residence_since',\n",
    "    'property',\n",
    "    'age_in_years',\n",
    "    'other_installment_plans',\n",
    "    'housing',\n",
    "    'nr_existing_credits_at_this_bank',\n",
    "    'job',\n",
    "    'nr_people_liable_to_provide_maintenance_for',\n",
    "    'telephone',\n",
    "    'foreign_worker',\n",
    "    'customer'\n",
    "]\n",
    "\n",
    "codes = {\n",
    "    'A11': 'less than 0 DM',\n",
    "    'A12': '0 - 200 DM',\n",
    "    'A13': 'greater than 200 DM',\n",
    "    'A14': 'no checking account',\n",
    "    'A30': 'no credits taken/ all credits paid back duly',\n",
    "    'A31': 'all credits at this bank paid back duly',\n",
    "    'A32': 'existing credits paid back duly till now',\n",
    "    'A33': 'delay in paying off in the past',\n",
    "    'A34': 'critical account/ other credits existing (not at this bank)',\n",
    "    'A40': 'car (new)',\n",
    "    'A41': 'car (used)',\n",
    "    'A42': 'furniture/equipment',\n",
    "    'A43': 'radio/television',\n",
    "    'A44': 'domestic appliances',\n",
    "    'A45': 'repairs',\n",
    "    'A46': 'education',\n",
    "    'A47': 'vacation - does not exist?',\n",
    "    'A48': 'retraining',\n",
    "    'A49': 'business',\n",
    "    'A410': 'others',\n",
    "    'A61': 'less than 100 DM',\n",
    "    'A62': '100 - 500 DM',\n",
    "    'A63': '500 - 1000 DM',\n",
    "    'A64': 'greater than 1000 DM',\n",
    "    'A65': 'unknown/ no savings account',\n",
    "    'A71': 'unemployed',\n",
    "    'A72': 'less than 1 year',\n",
    "    'A73': '1 - 4 years',\n",
    "    'A74': '4 - 7 years',\n",
    "    'A75': 'greater than 7 years',\n",
    "    'A91': 'male divorced/separated',\n",
    "    'A92': 'female divorced/separated/married',\n",
    "    'A93': 'male single',\n",
    "    'A94': 'male married/widowed',\n",
    "    'A95': 'female single',\n",
    "    'A101': 'none',\n",
    "    'A102': 'co-applicant',\n",
    "    'A103': 'guarantor',\n",
    "    'A121': 'real estate',\n",
    "    'A122': 'building society savings agreement/ life insurance',\n",
    "    'A123': 'car or other',\n",
    "    'A124': 'unknown / no property',\n",
    "    'A141': 'bank',\n",
    "    'A142': 'stores',\n",
    "    'A143': 'none',\n",
    "    'A151': 'rent',\n",
    "    'A152': 'own',\n",
    "    'A153': 'for free',\n",
    "    'A171': 'unemployed/ unskilled - non-resident',\n",
    "    'A172': 'unskilled - resident',\n",
    "    'A173': 'skilled employee / official',\n",
    "    'A174': 'management/ self-employed/highly qualified employee/ officer',\n",
    "    'A191': 'none',\n",
    "    'A192': 'yes, registered under the customers name',\n",
    "    'A201': 'yes',\n",
    "    'A202': 'no'\n",
    "}\n",
    "\n",
    "def process_data(df):\n",
    "    \n",
    "    df_ = df.copy()\n",
    "    \n",
    "    \n",
    "    # LABEL ENCODE ORDINAL CATEGORICAL FEATURES\n",
    "\n",
    "    # status_checking_account\n",
    "\n",
    "    df_['has_checking_account'] = (df_['status_checking_account']!= 'no checking account').astype(int)\n",
    "\n",
    "    status_checking_account_mapping = {'no checking account': 0,\n",
    "                                       'less than 0 DM': 1,\n",
    "                                       '0 - 200 DM': 2,\n",
    "                                       'greater than 200 DM': 3}\n",
    "\n",
    "\n",
    "    df_['status_checking_account'] = df_.status_checking_account.map(status_checking_account_mapping)\n",
    "\n",
    "\n",
    "    # savings_account_or_bonds\n",
    "\n",
    "    df_['known_savings_account_or_bonds'] = (df_['savings_account_or_bonds']!='unknown/ no savings account').astype(int)\n",
    "\n",
    "    savings_account_or_bonds_mapping = {'unknown/ no savings account': 0,\n",
    "                                       'less than 100 DM': 1,\n",
    "                                       '100 - 500 DM': 2,\n",
    "                                       '500 - 1000 DM': 3,\n",
    "                                       'greater than 1000 DM': 4}\n",
    "\n",
    "    df_['savings_account_or_bonds'] = df_.savings_account_or_bonds.map(savings_account_or_bonds_mapping)\n",
    "\n",
    "\n",
    "    # present_employment_since\n",
    "\n",
    "    df_['employed'] = (df_['present_employment_since']!='unemployed').astype(int)\n",
    "\n",
    "    present_employment_since_mapping = {'unemployed': 0,\n",
    "                                       'less than 1 year': 1,\n",
    "                                       '1 - 4 years': 2,\n",
    "                                       '4 - 7 years': 3,\n",
    "                                       'greater than 7 years': 4}\n",
    "\n",
    "    df_['present_employment_since'] = df_.present_employment_since.map(present_employment_since_mapping)\n",
    "\n",
    "    \n",
    "    \n",
    "    # MERGE SIMILAR CATEGORIES\n",
    "\n",
    "    df_['credit_history'] = df_['credit_history'].replace(\n",
    "        {'all credits at this bank paid back duly':'no credits taken/ all credits paid back duly'})\n",
    "\n",
    "\n",
    "    \n",
    "    # FEATURE ENGINEERING CONSIDERING FEATURE INTERACTION\n",
    "\n",
    "    df_['credit_amount/duration'] = df_.credit_amount/df_.duration_in_months\n",
    "    df_['age/credit_amount'] = df_.age_in_years/df_.credit_amount\n",
    "\n",
    "    df_['status_checking_account/age'] = df_.status_checking_account/df_.age_in_years\n",
    "    df_['savings_account_or_bonds/age'] = df_.savings_account_or_bonds/df_.age_in_years\n",
    "\n",
    "    df_['status_checking_account/credit_amount'] = df_.status_checking_account/df_.credit_amount\n",
    "    df_['savings_account_or_bonds/credit_amount'] = df_.savings_account_or_bonds/df_.credit_amount\n",
    "\n",
    "\n",
    "    \n",
    "    # HANDLE RARE CATEGORIES IN CATEGORICAL FEATURES\n",
    "    \n",
    "    categoricals = df_.select_dtypes(include=['object']).columns.tolist()\n",
    "        \n",
    "    for cat in categoricals:\n",
    "        frequencies = df_[cat].value_counts(normalize = True)\n",
    "        mapping = df_[cat].map(frequencies).to_numpy()\n",
    "        df_[cat] = df_[cat].mask(mapping < 0.02, 'other')\n",
    "\n",
    "        \n",
    "        \n",
    "    # CHANGE TYPE OF CATEGORICAL FEATURES\n",
    "    \n",
    "    df_[categoricals] = df_.select_dtypes('object').astype('category')\n",
    "    \n",
    "    \n",
    "    \n",
    "    # PROCESS TARGET FEATURE (type of customer: bad: 1, good: 0)\n",
    "    \n",
    "    df_['customer'] = (df_.customer == 2).astype(int)\n",
    "    \n",
    "    return df_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data downloaded from https://archive.ics.uci.edu/ml/datasets/Statlog+%28German+Credit+Data%29\n",
    "data = pd.read_csv('data/german.data', sep=' ', names=columns)\n",
    "data = data.replace(codes)\n",
    "data = process_data(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from interpret import show\n",
    "from interpret.data import ClassHistogram\n",
    "\n",
    "X = data.drop('customer', axis = 1)\n",
    "y = data.customer\n",
    "\n",
    "hist = ClassHistogram().explain_data(X, y, name = 'Train Data')\n",
    "show(hist)\n",
    "\n",
    "graphs = []\n",
    "graphs.append(hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from collections import Counter\n",
    "\n",
    "rus = RandomUnderSampler(random_state = 42)\n",
    "\n",
    "X_, y_ = rus.fit_resample(X, y)\n",
    "\n",
    "print(sorted(Counter(y_).items()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "seed = 42\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_, y_, test_size = 0.2, random_state = seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling and Explanations\n",
    "\n",
    "<a id='intrepret_ml'></a>\n",
    "\n",
    "# *1. Using Interpret ML*\n",
    "\n",
    "Interpret ML is an open-source package that incorporates state-of-the-art machine learning interpretability techniques under one roof.\n",
    "\n",
    "Project: https://github.com/interpretml/interpret\n",
    "\n",
    "Types of explanability algorithms available:\n",
    "- Glassbox Models:\n",
    "    - Inherently intelligible and explainable models (i.e Linear Models, Decision trees)\n",
    "    - New model called Explainable Boosting Machine (EBM)\n",
    "- Blackbox Models:\n",
    "    - Generate explanations for any machine learning model\n",
    "    - Uses interpretation techniques like LIME, SHAP, Partial Dependence, Morris Sensitivity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Glass Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.1. Explainable Boosting Classifier\n",
    "\n",
    "parameters description: https://github.com/interpretml/interpret/blob/develop/python/interpret-core/interpret/glassbox/ebm/ebm.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from interpret.glassbox import ExplainableBoostingClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "\n",
    "ebm = ExplainableBoostingClassifier(random_state = 42, \n",
    "                                    learning_rate = 0.01)\n",
    "\n",
    "cv = KFold(n_splits = 5, shuffle = True, random_state = 42)\n",
    "\n",
    "cv_ebm = cross_val_score(ebm, X_train, y_train, cv = cv, scoring = 'accuracy')\n",
    "\n",
    "print('Mean accuracy from CV: ', np.round(np.mean(cv_ebm), 2))\n",
    "\n",
    "ebm.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.2. Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from category_encoders.one_hot import OneHotEncoder\n",
    "from interpret.glassbox import LogisticRegression\n",
    "\n",
    "\n",
    "# Get features names after encoding\n",
    "ohe = OneHotEncoder(use_cat_names = True)\n",
    "ohe.fit(X_train)\n",
    "feature_names = ohe.feature_names\n",
    "\n",
    "\n",
    "lr = LogisticRegression(random_state = 42, \n",
    "                        solver = 'liblinear', \n",
    "                        C = 0.0133, \n",
    "                        penalty = 'l2',\n",
    "                        feature_names = feature_names)\n",
    "\n",
    "\n",
    "pipe_lr = Pipeline([\n",
    "    ('ohe', OneHotEncoder(use_cat_names = True)),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('lr', lr)])\n",
    "\n",
    "\n",
    "cv = KFold(n_splits = 5, shuffle = True, random_state = 42)\n",
    "\n",
    "\n",
    "cv_lr = cross_val_score(pipe_lr, X_train, y_train, cv = cv, scoring = 'accuracy')\n",
    "\n",
    "print('Mean accuracy from CV: ', np.round(np.mean(cv_lr), 2))\n",
    "\n",
    "pipe_lr.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.3. Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from interpret.perf import ROC\n",
    "\n",
    "acc_ebm = accuracy_score(y_test, ebm.predict(X_test))\n",
    "acc_lr = accuracy_score(y_test, pipe_lr.predict(X_test))\n",
    "\n",
    "print('Test accuracy: ')\n",
    "print('EBM: ', np.round(acc_ebm, 4))\n",
    "print('LR: ', np.round(acc_lr, 4))\n",
    "\n",
    "ebm_perf = ROC(ebm.predict_proba).explain_perf(X_test, y_test, name = 'Explainable Boosting Classifier')\n",
    "\n",
    "lr_perf = ROC(pipe_lr['lr'].predict_proba).explain_perf(X = pipe_lr[:2].transform(X_test),\n",
    "                                                        y = y_test, \n",
    "                                                        name = 'Logistic Regression')\n",
    "\n",
    "graphs += [ebm_perf, lr_perf]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.4. Global explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ebm_global = ebm.explain_global(name='Explainable Boosting Classifier')\n",
    "lr_global = pipe_lr['lr'].explain_global(name='Logistic Regression')\n",
    "\n",
    "\n",
    "graphs += [ebm_global, lr_global]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.5. Local explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ebm_local = ebm.explain_local(X_test[:10], y_test[:10], name = 'Explainable Boosting Classifier')\n",
    "\n",
    "lr_local = pipe_lr['lr'].explain_local(X = pipe_lr[:2].transform(X_test[:10]),\n",
    "                                       y = y_test[:10], \n",
    "                                       name = 'Logistic Regression')\n",
    "\n",
    "graphs += [ebm_local, lr_local]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Black-box Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1. SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "svm = SVC(random_state = 42, C = 0.001, gamma = 0.001, kernel = 'linear', probability = True)\n",
    "\n",
    "pipe_svm = Pipeline([\n",
    "    ('ohe', OneHotEncoder(use_cat_names = True)),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('svm', svm)])\n",
    "\n",
    "cv = KFold(n_splits = 5, shuffle = True, random_state = 42)\n",
    "\n",
    "cv_svm = cross_val_score(pipe_svm, X_train, y_train, cv = cv, scoring = 'accuracy')\n",
    "\n",
    "print('Mean accuracy from CV: ', np.round(np.mean(cv_svm), 2))\n",
    "\n",
    "pipe_svm.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2. RandomForestClassifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "rf = RandomForestClassifier(random_state = 42,\n",
    "                            bootstrap = False,\n",
    "                            max_depth = 10,\n",
    "                            max_features = 'auto',\n",
    "                            min_samples_leaf = 4,\n",
    "                            min_samples_split = 2,\n",
    "                            n_estimators = 130)\n",
    "\n",
    "\n",
    "pipe_rf = Pipeline([\n",
    "    ('ohe', OneHotEncoder(use_cat_names = True)),\n",
    "    ('rf', rf)\n",
    "])\n",
    "\n",
    "cv = KFold(n_splits = 5, shuffle = True, random_state = 42)\n",
    "        \n",
    "cv_rf = cross_val_score(pipe_rf, X_train, y_train, cv = cv, scoring = 'accuracy')\n",
    "\n",
    "print('Mean accuracy from CV: ', np.round(np.mean(cv_rf), 2))\n",
    "\n",
    "pipe_rf.fit(X_train, y_train);\n",
    "\n",
    "acc_rf = accuracy_score(y_test, pipe_rf.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.4. Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from interpret.perf import ROC\n",
    "\n",
    "acc_svm = accuracy_score(y_test, pipe_svm.predict(X_test))\n",
    "acc_rf = accuracy_score(y_test, pipe_rf.predict(X_test))\n",
    "\n",
    "print('Test accuracy: ')\n",
    "print('SVM: ', np.round(acc_svm, 4))\n",
    "print('RF: ', np.round(acc_rf, 4))\n",
    "\n",
    "\n",
    "svm_perf = ROC(pipe_svm['svm'].predict_proba).explain_perf(X = pipe_svm[:2].transform(X_test),\n",
    "                                                           y = y_test,\n",
    "                                                           name = 'SVM')\n",
    "\n",
    "rf_perf = ROC(pipe_rf['rf'].predict_proba).explain_perf(X = pipe_rf[:1].transform(X_test),\n",
    "                                                        y = y_test,\n",
    "                                                        name = 'Random Forest Classifier')\n",
    "\n",
    "graphs += [svm_perf, rf_perf]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.5. Global explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from interpret.blackbox import MorrisSensitivity\n",
    "from interpret.blackbox import PartialDependence\n",
    "\n",
    "def blackbox_global_explanations(predict_fn, data, feature_names, name):\n",
    "    \n",
    "    # Morris Sensitivity\n",
    "    \n",
    "    sensitivity = MorrisSensitivity(predict_fn, data, feature_names = feature_names)\n",
    "    sensitivity_global = sensitivity.explain_global(name = name + \" - Morris Sensitivity\")\n",
    "    \n",
    "    # Partial Dependence\n",
    "    \n",
    "    pdp = PartialDependence(predict_fn, data, feature_names = feature_names)\n",
    "    pdp_global = pdp.explain_global(name = name + \" - Partial Dependence\")\n",
    "    \n",
    "                                    \n",
    "    return sensitivity_global, pdp_global"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_sensitivity_global, svm_pdp_global = blackbox_global_explanations(predict_fn = pipe_svm['svm'].predict_proba,\n",
    "                                                                      feature_names = feature_names,\n",
    "                                                                      data = pipe_svm[:2].transform(X_train), \n",
    "                                                                      name = \"SVM\")\n",
    "\n",
    "\n",
    "rf_sensitivity_global, rf_pdp_global = blackbox_global_explanations(predict_fn = pipe_rf['rf'].predict_proba,\n",
    "                                                                    feature_names = feature_names,\n",
    "                                                                    data = pipe_rf[:1].transform(X_train),\n",
    "                                                                    name = \"RF\")\n",
    "\n",
    "\n",
    "graphs += [svm_sensitivity_global, svm_pdp_global, rf_sensitivity_global, rf_pdp_global]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.6. Local explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from interpret.blackbox import LimeTabular\n",
    "from interpret.blackbox import ShapKernel\n",
    "\n",
    "def blackbox_local_explanations(predict_fn, X_train, X_explain, y_explain, feature_names, name):\n",
    "    \n",
    "    \n",
    "    # Lime\n",
    "    \n",
    "    lime = LimeTabular(predict_fn, data = X_train, random_state = seed, feature_names = feature_names)\n",
    "    lime_local = lime.explain_local(X_explain, y_explain, name = name + ' - Lime')\n",
    "\n",
    "\n",
    "    # Shap\n",
    "    \n",
    "    X_train_ = np.median(X_train, axis=0).reshape(1, -1)\n",
    "\n",
    "    shap = ShapKernel(predict_fn, data = X_train_, feature_names = feature_names)\n",
    "    shap_local = shap.explain_local(X_explain, y_explain, name = name + ' - Shap')\n",
    "\n",
    "    \n",
    "    return lime_local, shap_local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_lime_local, svm_shap_local = blackbox_local_explanations(predict_fn = pipe_svm['svm'].predict_proba, \n",
    "                                                             X_train = pipe_svm[:2].transform(X_train), \n",
    "                                                             X_explain = pipe_svm[:2].transform(X_test[:5]),\n",
    "                                                             y_explain = y_test[:5], \n",
    "                                                             feature_names = feature_names, \n",
    "                                                             name = 'SVM')\n",
    "\n",
    "                                                             \n",
    "rf_lime_local, rf_shap_local = blackbox_local_explanations(predict_fn = pipe_rf['rf'].predict_proba,\n",
    "                                                           X_train = pipe_rf[:1].transform(X_train), \n",
    "                                                           X_explain = pipe_rf[:1].transform(X_test[:5]),\n",
    "                                                           y_explain = y_test[:5],\n",
    "                                                           feature_names = feature_names, \n",
    "                                                           name = 'Random Forest')\n",
    "\n",
    "\n",
    "graphs += [svm_lime_local, svm_shap_local, rf_lime_local, rf_shap_local]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ['Explainable Boosting Classifier', 'Logistic Regression', 'SVM', 'Random Forest Classifier']\n",
    "acc_train = np.round([np.mean(cv_ebm), np.mean(cv_lr), np.mean(cv_svm), np.mean(cv_rf)], 4)\n",
    "acc_test = np.round(np.array([acc_ebm, acc_lr, acc_svm, acc_rf]), 4)\n",
    "\n",
    "summary = pd.DataFrame({'Models': models, 'Train Accuracy': acc_train, 'Test Accuracy': acc_test})\n",
    "summary = summary.sort_values(by = 'Test Accuracy', ascending = False)\n",
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4. Interpret ML Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "show(graphs, share_tables=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='intrepret_ml_community'></a>\n",
    "\n",
    "# *2. Using Interpret ML Community*\n",
    "\n",
    "Interpret-Community is an experimental repository that extends Interpret with additional capabilities. \n",
    "\n",
    "Project: https://github.com/interpretml/interpret-community"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Raw features transformation\n",
    "\n",
    "You can pass your feature transformation pipeline to the explainer to receive explanations in terms of the raw features before the transformation (rather than engineered features). If you skip this, the explainer provides explanations in terms of engineered features.\n",
    "\n",
    "The format of supported transformations is same as the one described in sklearn-pandas. In general, any transformations are supported as long as they operate on a single column and are therefore clearly one to many.\n",
    "\n",
    "We can explain raw features by either using a sklearn.compose.ColumnTransformer or a list of fitted transformer tuples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Feature transformations and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from interpret.ext.blackbox import KernelExplainer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "\n",
    "# Create transformers for numeric and categorical features\n",
    "\n",
    "numeric_features = X_train.select_dtypes(np.number).columns.tolist()\n",
    "categorical_features = X_train.select_dtypes('category').columns.tolist()\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler())])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)])\n",
    "\n",
    "\n",
    "# Create pipeline with the transformer and the model\n",
    "\n",
    "svm = SVC(random_state = 42, probability = True)\n",
    "\n",
    "clf = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                      ('classifier', svm)])\n",
    "\n",
    "\n",
    "model = clf.fit(X_train, y_train)\n",
    "\n",
    "print('accuracy: ', np.round(accuracy_score(y_test, model.predict(X_test)), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tabular_explainer = KernelExplainer(clf.steps[-1][1],\n",
    "                                     initialization_examples = X_train,\n",
    "                                     features = X_train.columns.tolist(),\n",
    "                                     classes = ['good customer', 'bad customer'],\n",
    "                                     transformations = preprocessor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1. Global explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Passing in test dataset for evaluation examples - note it must be a representative sample of the original data\n",
    "# X_train can be passed as well, but with more examples explanations will take longer although they may be more accurate\n",
    "\n",
    "global_explanation = tabular_explainer.explain_global(X_test)\n",
    "\n",
    "global_explanation.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2. Local explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_local_explanations(sorted_local_importance_names, sorted_local_importance_values):\n",
    "    \n",
    "    x = np.array(sorted_local_importance_values[1][0])\n",
    "\n",
    "    sorting = np.argsort(-np.abs(x))[:15]\n",
    "    \n",
    "    y = np.array(sorted_local_importance_names[1][0])\n",
    "    \n",
    "    hue = (x > 0)*1\n",
    "\n",
    "    plt.figure(figsize = (8,8))\n",
    "    sns.barplot(x = x[sorting], y = y[sorting], hue = hue[sorting]);\n",
    "    \n",
    "    \n",
    "sample_idx = 7\n",
    "sample = X_test.reset_index(drop=True).iloc[[sample_idx]]\n",
    "\n",
    "local_explanation = tabular_explainer.explain_local(sample)\n",
    "\n",
    "print('True class: ', y_test.iloc[sample_idx])\n",
    "print('Predicted proba: ', np.round(model.predict_proba(sample)[0][1], 2))\n",
    "\n",
    "sorted_local_importance_names = local_explanation.get_ranked_local_names()\n",
    "sorted_local_importance_values = local_explanation.get_ranked_local_values()\n",
    "\n",
    "plot_local_explanations(sorted_local_importance_names, sorted_local_importance_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.3. Explanation Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from interpret_community.widget import ExplanationDashboard\n",
    "\n",
    "ExplanationDashboard(global_explanation, model, datasetX = X_test, true_y = y_test);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='lime'></a>\n",
    "\n",
    "# *3. Using LIME*\n",
    "\n",
    "Project: https://github.com/marcotcr/lime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Feature transformations and model\n",
    "\n",
    "\n",
    "One hot encode the categorical features for the classifier, but not for the explainer. The explainer should have the categorical features label encoded instead, because it must make sure that a categorical feature only has one possible value when perturbing the data. For example, if after one hot encoding the data [0,1] is female and [1,0] is male, a perturbation [1,1] caused by LIME algorithm would not make sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lime\n",
    "import lime.lime_tabular\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "\n",
    "X_enc = X_.copy()\n",
    "\n",
    "\n",
    "# LABEL ENCODE CATEGORICAL FEATURES\n",
    "# build dictionary to save the correspondence between labels and the original string classes\n",
    "\n",
    "categorical_features = X_enc.select_dtypes('category').columns.tolist()\n",
    "\n",
    "categorical_names = {}\n",
    "\n",
    "for feature in categorical_features:\n",
    "    le = LabelEncoder()\n",
    "    le.fit(X_enc[feature])\n",
    "    X_enc[feature] = le.transform(X_enc[feature])\n",
    "    categorical_names[feature] = le.classes_\n",
    "    \n",
    "\n",
    "# Get index of categorical features   \n",
    "feature_names = X_enc.columns.tolist()\n",
    "categorical_features_idx = pd.Series(feature_names).isin(categorical_features)\n",
    "categorical_features_idx = np.arange(len(feature_names))[categorical_features_idx].tolist()\n",
    "\n",
    "\n",
    "# SPLIT DATA INTO TRAIN AND TEST SETS\n",
    "\n",
    "X_train_enc, X_test_enc, y_train, y_test = train_test_split(X_enc, y_, test_size = 0.2, random_state = 42)\n",
    "\n",
    "\n",
    "# ONE HOT ENCODE THE DATA FOR THE CLASSIFIER\n",
    "\n",
    "ohe = OneHotEncoder(handle_unknown = 'ignore')\n",
    "\n",
    "X_train_ohe = ohe.fit_transform(X_train_enc)\n",
    "X_test_ohe = ohe.transform(X_test_enc)\n",
    "\n",
    "rf = RandomForestClassifier(random_state = 42)\n",
    "rf.fit(X_train_ohe, y_train);\n",
    "\n",
    "\n",
    "print('accuracy: ', np.round(accuracy_score(y_test, rf.predict(X_test_ohe)), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LimeTabularExplainer needs a training set because LIME computes statistics on each feature. If the feature is numerical, LIME computes the mean and std, and discretizes it into quartiles. If the feature is categorical, LIME computes the frequency of each value.\n",
    "\n",
    "These statistics are computed for two things:\n",
    "\n",
    "1. To scale the data, so that we can meaningfully compute distances when the attributes are not on the same scale\n",
    "2. To sample perturbed instances - which we do by sampling from a Normal(0,1), multiplying by the std and adding back the mean.\n",
    "\n",
    "\n",
    "Find documentation on parameters info here: https://lime-ml.readthedocs.io/en/latest/lime.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train data should be label encoded not one hot encoded\n",
    "\n",
    "np.random.seed(1)\n",
    "\n",
    "explainer = lime.lime_tabular.LimeTabularExplainer(X_train_enc.to_numpy(),\n",
    "                                                   feature_names = feature_names,\n",
    "                                                   class_names = ['good customer', 'bad customer'],\n",
    "                                                   categorical_features = categorical_features_idx, \n",
    "                                                   categorical_names = categorical_names, \n",
    "                                                   kernel_width = 4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "sample_idx = 5\n",
    "sample = X_test_enc.reset_index(drop = True).to_numpy()[sample_idx]\n",
    "\n",
    "\n",
    "# the predict function first transforms the data into the one-hot representation\n",
    "predict_fn = lambda x: rf.predict_proba(ohe.transform(x)).astype(float)\n",
    "\n",
    "\n",
    "exp = explainer.explain_instance(sample, predict_fn = predict_fn)\n",
    "exp.show_in_notebook(show_all = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='shap'></a>\n",
    "\n",
    "# *4. Using SHAP*\n",
    "\n",
    "Project: https://github.com/slundberg/shap  \n",
    "Documentation: https://shap.readthedocs.io/en/latest/#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "import lightgbm as lgb\n",
    "\n",
    "# Initialize your Jupyter notebook with initjs(), otherwise you will get an error message when plotting the graphs\n",
    "shap.initjs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_train = lgb.Dataset(X_train, label = y_train)\n",
    "d_test = lgb.Dataset(X_test, label = y_test)\n",
    "\n",
    "\n",
    "params = {\n",
    "    \"num_iterations\": 10000,\n",
    "    \"min_data_in_leaf\": 50,\n",
    "    \"max_bin\": 50,\n",
    "    \"learning_rate\": 0.0001,\n",
    "    \"boosting_type\": \"gbdt\",\n",
    "    \"objective\": \"binary\",\n",
    "    \"metric\": \"binary_logloss\",\n",
    "    \"num_leaves\": 11,\n",
    "    \"max_depth\": 10,\n",
    "    \"verbose\": -1,\n",
    "    \"min_data\": 100,\n",
    "    \"boost_from_average\": True,\n",
    "    \"seed\": 42,\n",
    "    \"feature_fraction\": 0.4\n",
    "}\n",
    "\n",
    "model = lgb.train(params, d_train, valid_sets = [d_test], early_stopping_rounds = 50, verbose_eval = 1000)\n",
    "\n",
    "print()\n",
    "print('train accuracy: ', np.round(accuracy_score(y_train, model.predict(X_train).round(0)), 4))\n",
    "print('test accuracy: ', np.round(accuracy_score(y_test, model.predict(X_test).round(0)), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2. Explanations\n",
    "\n",
    "### Explain entire dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.TreeExplainer(model)\n",
    "shap_values = explainer.shap_values(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize a single prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_idx = 1\n",
    "sample = X_test.reset_index(drop = True).iloc[[sample_idx]]\n",
    "\n",
    "print('True class: ', y_test.reset_index(drop = True).iloc[sample_idx])\n",
    "print('Predicted proba: ', np.round(model.predict(sample)[0], 2))\n",
    "\n",
    "shap.force_plot(explainer.expected_value[1], shap_values[1][sample_idx,:], sample)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize many predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.force_plot(explainer.expected_value[1], shap_values[1][:1000,:], X_train.iloc[:1000,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SHAP summary plot\n",
    "\n",
    "Density scatter plot of SHAP values for each feature to identify how much impact each feature has on the model output. Features are sorted by the sum of the SHAP value magnitudes across all samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values, X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SHAP Dependence Plots\n",
    "\n",
    "SHAP dependence plots show the effect of a single feature across the whole dataset. They plot a feature’s value vs. the SHAP value of that feature across many samples. SHAP dependence plots are similar to partial dependence plots, but account for the interaction effects present in the features, and are only defined in regions of the input space supported by data. The vertical dispersion of SHAP values at a single feature value is driven by interaction effects with other features, and another feature is chosen for coloring to highlight possible interactions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_features = X_test.select_dtypes(np.number).columns.tolist()\n",
    "numerical_features_idx = X_test.columns.isin(numerical_features)\n",
    "\n",
    "for name in numerical_features:\n",
    "    shap.dependence_plot(name, shap_values[1][:,numerical_features_idx], X_test[numerical_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
